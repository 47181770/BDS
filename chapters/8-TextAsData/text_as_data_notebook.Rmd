---
title: "R Notebook"
output: html_notebook
---

#### Info retrieval ####

The tm library (and related plugins) is R's ecosystem for text mining. For an intro see http://cran.r-project.org/web/packages/tm/vignettes/tm.pdf


The way file input works with tm is you create a reader function, depending on document type.  Each of the reader functions have arguments elem, language, id (see ?readPlain,?readPDF,etc). I wrap another function around them to specify these arguments.

```{r}
## for example, a reader to input plain text files 
## (Note: there are many other ways to do this)
readerPlain <- function(fname){
				readPlain(elem=list(content=readLines(fname)), 
							id=fname, language='en') }
## test it on this script
## (the file name will change depending on where you store stuff).
rcode <- readerPlain("text.R")
rcode # this is the tm 'PlainTextDocument'
content(rcode)[1:21] # this is the actual text part

```

## *** Reading PDFs ***

From the tm docs: "Note that this PDF reader needs the tool pdftotext installed and accessible on your system, available as command line utility in the Poppler PDF rendering library (see http://poppler.freedesktop.org/)." this appears to be the default on windows.

```{r}
## we'll create a 'reader' function to interpret pdfs, 
## using tm's readPDF (see help(readPDF) examples)

readerPDF <- function(fname){
		txt <- readPDF(control = list(text = "-layout -enc UTF-8"))(elem=list(uri=fname), 
															id=fname, language='en')
		return(txt)
	}
```

For the following to work, in your working directory you'll need to point to wherever you've stored the lectures.

```{r}
## apply to all the lectures (assuming you are running from MBAmaterials/examples)
files <- Sys.glob("lectures/*.pdf") 
```

NOTE: Sys.glob just expands file names from 'wildcards' takes time!  this would be easy to do  distributed via clusterApply or MapReduce.

```{r}
#Apply pdf reader function to lecture slides
notes <- lapply(files, readerPDF)

## some string manipulation to get nice names
names(notes) = sub('.pdf', '', substring(files,first=37))
names(notes)
writeLines(content(notes[[1]])[1]) # the cover slide
```

Convert the string type of the imported pdf's to a common string type.

```{r}
#String type conversion to ASCII
for(i in 1:11) 
 content(notes[[i]]) <- 
 	iconv(content(notes[[i]]), from="UTF-8", to="ASCII", sub="")
```

To begin text mining with the tm package we must now bring the text into the tm ecosystem.

```{r}
## once you have a bunch of docs in a vector, you 
## create a text mining 'corpus' with: 
docs <- Corpus(VectorSource(notes))
```

We can clean the text with a few tools from the tm package.

```{r}
## you can then do some cleaning here
## tm_map just maps some function to every document in the corpus

## make everything lowercase
docs <- tm_map(docs, content_transformer(tolower))

## remove numbers
docs <- tm_map(docs, content_transformer(removeNumbers))

## remove punctuation
docs <- tm_map(docs, content_transformer(removePunctuation))

## remove stopword.  be careful with this: one's stopwords are anothers keywords.
docs <- tm_map(docs, content_transformer(removeWords), stopwords("SMART"))

## remove excess white-space
docs <- tm_map(docs, content_transformer(stripWhitespace))

```

With the data clean, create a Document-Term-Matrix in which each row represents a document and each column counts the appearance of the specific word in the document.

```{r}
## create a doc-term-matrix
dtm <- DocumentTermMatrix(docs)
dtm # 11 documents, > 4K terms
```

Finally, drop those terms that only occur in one or two lectures. This is a common step: you the noise of rare terms to overwhelm things, and there is nothing to learn if a term occured once. Below removes those terms that have count 0 in >75% of docs. This is way more harsh than you'd usually do (but we only have 11 docs here) .75*11 is 8.25, so this will remove those with zeros in 9+ docs. ie, it removes anything that doesn't occur in at least 3 docs.

```{r}
#Remove words that appear in less than 75% of documents
dtm <- removeSparseTerms(dtm, 0.75)
dtm # now near 700 terms
```

We have now created a sparse matrix that can be treated as any other high dimensional matrix. We show this through a few commands.

```{r}
## These are special sparse matrices.  
class(dtm)

## You can inspect them:
inspect(dtm[1:5,1:8])

## find words with greater than a min count
findFreqTerms(dtm,100)

## or grab words whose count correlates with given words
findAssocs(dtm, "lasso", .9) 
```

#### Text Regression ####

In the textir package, we will examine the congress109 dataset which shows common tokens used in the 109th congress. The dataset already comes tokenized in groups of words.

```{r}
#Import textir library and data
library(textir)
data(congress109)
```

```{r}
#See structure of the dataset
str(congress109Counts)

#Examine a piece of the dataset
congress109Counts[c("Barack Obama","John Boehner"),995:998]
```

The regression we will now perform will be a variation of partial least squares. We belive that the constituents for a congress member provide some sort of a signal of the congress members beliefs, we can use the variable repshare (proportion of two party vote) as a substitute for ideology and partisanship of the congress member.

```{r}
## Perform pls regression

#Define covariates
f <- congress109Counts

#Define response variable
y <- congress109Ideology$repshare

#Perform pls
slant <- pls(f, y, K=3)
```

Plot the first three PLS iterations. Color points to signify party.

```{r}
#Create plots
for(k in 1:3)
	plot(slant$y, slant$fitted[,k], ylim=c(-.05,.85), xlab="", ylab="",
	 main=sprintf("PLS(%d)", k), 
   		pch=20, col=c(4,3,2)[congress109Ideology$party], bty="n")
mtext(side=1, "repshare", outer=TRUE, line=-1.25)
mtext(side=2, "fitted", outer=TRUE, line=-1.25)

```

To get good model results, conduct and OOS experiment for PLS.

```{r}
## OOS experiment

#Establish cross validation folds
foldid <- sample.int(5, nrow(f), replace=TRUE)
#Create matrix to store OOS results
OOS <- matrix(nrow=5, ncol=10)
#Construct for-loop to run experiment
for(b in 1:5){
	#Print iteration
  print(b)
  #Select all folds except one for model training
	fb <- f[,colSums(f[foldid!=b,])!=0]
	#Run pls
	for(k in 1:10){
		gpls <- pls(x=fb[foldid!=b,], y=y[foldid!=b], K=k)
		OOS[b,k] <- 
			mean( (y[foldid==b] - predict(gpls, fb[foldid==b,], K=k))^2 )
	}
}
#Get cross validation results
cvm <- apply(OOS,2,mean)
cvs <- apply(OOS,2,sd)
OOS <- as.data.frame(OOS)
names(OOS) <- 1:10
```

An alternative to pls would be conducting a lasso regression.

```{r}
# lasso instead of PLS
lassoslant <- cv.gamlr(congress109Counts>0, y)
B <- coef(lassoslant$gamlr)[-1,]
sort(round(B[B!=0],4))
```

Let's compare the effectiveness of pls and lasso.

```{r}
# compare
boxplot(OOS, ylab="mean squared error", xlab="K", col="red", log="y", main="", 
	ylim=c(0.01,2))
mtext(side=3, "PLS", line=2)
plot(lassoslant, log="y", main="", ylim=c(0.01,2)) 
mtext(side=3, "lasso", line=2)
```


#### Topic Models ####

To illustrate an example of text factorization, we look at restuarant review dataset from we8there.com. Each aspect of the restuarant is rated on a 5 point scale (5 is the best) and is accompanied by a written review. Our goal is to see if we can get the raters scores for each category by factorizing their written review.

```{r}
#Import textir library and data
library(textir)
data(we8there)
```

Perform PCA.

```{r}
## Perform PCA

#Pull word tokens into a matrix x
x <- we8thereCounts

#Perform PCA
pca <- prcomp(x, scale=TRUE) # can take a long time

#Do PCA transformation on a few rows of the matrix x
v <- predict(pca)[,1:4]
```
```{r}
#Plot PC1 and overall rating
boxplot(v[,1] ~ we8thereRatings$Overall, xlab="overall rating", ylab="PC1 score")

```

While PCA can be effective, topic models for analyzing text data have become increasingly popular in recent times due to better interpretability of the factorization. We will now demonstrate how to implement a topic model.

```{r}
#Import maptx library to topic model tools
library(maptpx)
```

To work with maptpx package, need to convert our usual sparse matrix to that which is interpreted by the slam packages.

```{r}
## you need to convert from a Matrix to a `slam' simple_triplet_matrix
## luckily, this is easy.
x <- as.simple_triplet_matrix(we8thereCounts)
```
```{r}
# to fit, just give it the counts, number of `topics' K, and any other args
tpc <- topics(x,K=10)
```

When choosing the number of topics. If you supply a vector of topic sizes, it uses a Bayes factor to choose (BF is like exp(-BIC), so you choose the bigggest BF) the algo stops if BF drops twice in a row.

```{r}
tpcs <- topics(x,K=5*(1:5), verb=1) # it chooses 10 topics 
```

Interpretation of the model works by performing the same PCA top down or bottom up approach. We can view thee model summary by

```{r}
# summary prints the top `n' words for each topic,
# under ordering by `topic over aggregate' lift:
#    the topic word prob over marginal word prob.
summary(tpcs, n=10) 
# this will promote rare words that with high in-topic prob
```

Alternatively, you can look at words ordered by simple in-topic pro the topic-term probability matrix is called 'theta', and each column is a topi we can use these to rank terms by probability within topics.

```{r}
rownames(tpcs$theta)[order(tpcs$theta[,1], decreasing=TRUE)[1:10]]
rownames(tpcs$theta)[order(tpcs$theta[,2], decreasing=TRUE)[1:10]]
```

Bocplot the we8there overal rating with the topic scores.

```{r}
boxplot(tpcs$omega[,1] ~ we8thereRatings$Overall, col="gold", xlab="overall rating", ylab="topic 1 score")
boxplot(tpcs$omega[,2] ~ we8thereRatings$Overall, col="pink", xlab="overall rating", ylab="topic 2 score")
```

We can now perform a topic regression. This is similar to the PC regression in that we first fit a topics model and use the reuslts in a regression.

```{r}
#Import gamlr library
library(gamlr)

### First topics regression without cross validation

## omega is the n x K matrix of document topic weights
## i.e., how much of each doc is from each topic
## we'll regress overall rating onto it
stars <- we8thereRatings[,"Overall"]
tpcreg <- gamlr(tpcs$omega, stars, lmr=1e-3)
# number of stars up or down for moving up 10\% weight in that topic
drop(coef(tpcreg))*0.1
```


Now perform regression with cross validation.

```{r}
#Topics regression with cross validation.
regtopics.cv <- cv.gamlr(tpcs$omega, stars, lmr=1e-3)

#Also perform regression on un-topic transformed data for comparison.
# aka token regression
regwords.cv <- cv.gamlr(we8thereCounts, stars)
```

Compare the topic and token regressions.

```{r}
par(mfrow=c(1,2))

#Topic regression plot (first plot)
plot(regtopics.cv, ylim=c(1,2), xlab="log lamba", ylab="mean squared error")

#Token regression plot (second plot)
plot(regwords.cv, ylim=c(1,2), xlab="log lamba", ylab="mean squared error")

```


#### Multinomial Text Regression ####

Let's walk through an example of multinomial text regression. We will return to the we8there data to use the 5 categories of ratings to predict the vector word counts for various words. This will be multinomial inverse regression in the sense that we are trying to associate rating categories with word types. This will utilize the distrom package, thus we will first need to make a local cluster.

```{r}
## cl=NULL instead implies a serial run. 
cl <- makeCluster(detectCores())
```
```{r}
## small nlambda for a fast example
fits <- dmr(cl, we8thereRatings, 
			we8thereCounts, bins=5,nlambda=10, lmr=1e-3)

#End local cluster
stopCluster(cl) # usually a good idea

```
```{r}
## plot fits for a few individual terms
terms <- c("first date","chicken wing",
			"ate here", "good food",
			"food fabul","terribl servic")
par(mfrow=c(2,3))
for(j in terms)
{ 	plot(fits[[j]]); mtext(j,font=2,line=2) }
 
```
```{r}
## extract coefficients
B <- coef(fits)
mean(B[-1,]==0) # sparsity in loadings
## some big loadings on `overall'
B[2,order(B[2,])[1:10]]
B[2,order(-B[2,])[1:10]]
```

We connect the idea of multinomial inverse regression (MNIR) to PC scores. Thus, if we wanted to generate a score for the degree to which some text is commonly associated with some attribute of a document, we seek to generate a MNIR projection.

```{r}
## do MNIR projection onto factors
z <- srproj(B,we8thereCounts)

## fit a fwd model to the factors
summary(fwd <- lm(we8thereRatings$Overall ~ z)) 

## truncate the fwd predictions to our known range
fwd$fitted[fwd$fitted<1] <- 1
fwd$fitted[fwd$fitted>5] <- 5
## plot the fitted rating by true rating
par(mfrow=c(1,1))
plot(fwd$fitted ~ factor(we8thereRatings$Overall), 
	varwidth=TRUE, col="lightslategrey")

```

#### Collaborative Filtering ####

Collaborative filtering is the process of predicting a person's future choices from their and others' past choices. Essentially, we will look to see if person a is similar to person b, and if they are in fact similar, recommend to person a stuff of person b.

We will use the lastfm dataset to illustrate the use of collaborative filtering by developing a set of association rules. This dataset looks at the music playlists of lastfm users.

```{r}
#Read in data
lastfm <- read.csv("lastfm.csv")

#View data structure
str(lastfm)
```

Develop association rules with the arules package.

```{r}
## Use the a-rules package for association rules
library(arules)
```

There is an entire ecosystem of packages around arules you need to first create a list of baskets: vectors of items by consumer.

```{r}
## Here's how we do the formatting here:
## split data into a list of artists for each user
playlists <- split(x=lastfm$artist, f=lastfm$user)
## re-move artist repetition in these lists
playlists <- lapply(playlists, unique)
## tell R to treat this as a special arules `transactions' class.
playtrans <- as(playlists, "transactions")
```

Find association rules.

```{r}
## now apply the actual 'apriori' algorithm
# you can add a list of arguments called 'parameter'.  Here, we look at
# only rules with support > .01 & confidence >.5 & length (# artists) <= 3
musicrules <- apriori(playtrans, 
	parameter=list(support=.01, confidence=.5, maxlen=3))
```

Loom at the discovered rules.

```{r}
## take a look
inspect(musicrules)
## Choose any subset you want. 
inspect(subset(musicrules, subset=lift > 5))
inspect(subset(musicrules, subset=confidence > 0.6))
inspect(subset(musicrules, subset=support > .02 & confidence > 0.6))
inspect(subset(musicrules, subset=lhs%in%"t.i."))
```

The topics modeling can provide similar results as a robust modeling approach. We can suggest products of high probability in whatever topics it appears our user favors.

```{r}
#Perform topic model
library(maptpx)
lastfm$user <- factor(lastfm$user)

x <- simple_triplet_matrix(i=as.numeric(lastfm$user), 
		j=as.numeric(lastfm$artist), v=rep(1,nrow(lastfm)), 
		nrow = nlevels(lastfm$user), ncol = nlevels(lastfm$artist),
        dimnames = list(levels(lastfm$user), levels(lastfm$artist)))

tpcs <- topics(x, K=5*(1:5), verb=1)

```

```{r}
summary(tpcs)
```

